{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 0,
        "width": 12
       },
       "report_default": {}
      }
     }
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Mentor-Mentee Recommendation System\n",
    "### Nidhi Singh\n",
    "### 8th September 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Problem Statement\n",
    "\n",
    "* Design a system for a user who is looking for an expert(s) based on his certain preferences. The experts are refered to as mentors and the expert seekers are mentees. \n",
    "* In our scenario, mentors are authors in DBLP(Database and lnaguage programming) bibliographic reference data and their expertise is inferred from title of their publications.\n",
    "* Mentees are users of the system who are recommended Mentors based on their captured preferences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 17,
        "hidden": false,
        "row": 4,
        "width": 12
       },
       "report_default": {}
      }
     }
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## System Process Flow\n",
    "\n",
    "\n",
    "![alt text](images/RecSysFlow.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Structure of DBLP data set\n",
    "\n",
    "![alt text](images/pub_description.png \"dblp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Parse and store DBLP \n",
    "\n",
    "* DBLP, a data set for bibliographic information on computer science publications \n",
    "* Our system uses the latest copy which indexes 3.7 million publication written in English by ~2 million authors.\n",
    "* We utilize Python's SAX xml parser to read the required fields for our scenario and store it in appropriate tables.\n",
    "* PostGreSQL is used for storing the parsed data.\n",
    "\n",
    "Key challenge\n",
    "- Author disambiguation - Authors with same name in the collection and also few Publications with same name authors\n",
    "Since we did not capture extra information of the authors which could help in disambiguation, our system assumes author name to be unique.\n",
    "- Publication fields duplication - Publications with multiple years\n",
    "We assume Publication attributes to be unique\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Topic Extraction\n",
    "DBLP data set does not contain keywords, paper abstract or full text, so the topics are extracted from titles.\n",
    "We use Python's NLTK package to perfrom text transformation and corpus preparation.\n",
    "![alt text](images/topics.png \"dblp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Corpus Preparation\n",
    "There is a main design decisions that entail corpus preparation in Information Retrieval:\n",
    "* Document based - this approach first ranks documents in the corpus given a query topic and then find associate candidates. So corpus comprises of every document, in our case, publication titles are documents.\n",
    "* Candidate based - in this approach we directly model the profile of the candidate based on all documents associated with the candidate and estimate ranking score according to profile in response to a user query.\n",
    "\n",
    "Since our system is query independent, we choose candidate based model. Also for simplification, as the topics extracted could be represented as expertise for the candidate. Furthermore, LDA algorithm works better for longer documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Corpus Preparation and Topic modeling\n",
    "* We identify and collect all publication titles of the author\n",
    "* Create a single document for each author\n",
    "* Combine all documents to create corpus\n",
    "\n",
    "* We use Python's Gensim's package LDA (Latent Dirichlet Allocation Model) for topic modeling on the corpus.\n",
    "* We choose number of topics as 20 and iteration passes as 5.\n",
    "    * number of topics were judged by running few iterations with 10, 15 and 20. 20 gave better distribution of terms over topics.\n",
    "    * 5 for no. of passes is chosen randomly as covergence point was computationally intensive to calculate.\n",
    "* LDA result of topics is further used to identify probable topics for each document, this is then saved as author-topic probability.\n",
    "* This choice is influenced by :\n",
    "    * LDA produces interpretable,semantically coherent topics, which can be examined by listing the most probable words for each topic.\n",
    "    * Gensim LDA has a multi-core variant, since we have 1.9 million documents in our corpus and topic modeling is computatinally intensive, this choice is vital. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Author expert profile\n",
    "Expertise can be defined as combination of several topics, hence a candidate can be represented in terms of mixing proportions of multiple topics.\n",
    "* We take the results of document-topic distribution as the input to the expert model for the author.\n",
    "* The topics are considered as the expertise topics of associated probabilities.\n",
    "* Expertise level can be further calculated by :\n",
    "    * number of total publications, publication per topic is more useful but not available in our scenario.\n",
    "    * co-authorship - each publication is authored by one or more authors, less the number of authors more the expert level of the author.\n",
    "    So if paper has 4 author, each author gets a co-author ship score of 1/4 (0.25)\n",
    "\n",
    "* We construct author profile feature vector, with topics as features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Mentor recommendations\n",
    "* For a mentee to be recommended a mentor, we need to model their preferences. In our case, we have identified a list of topics from the corpus. These can be used as topics to be get user preference.\n",
    "* From this preference, we construct a mentee preference feature vector.\n",
    "* To identify mentors similar to preference , there are 2 key decisions:\n",
    "    * similarity measure - we use cosine similarity\n",
    "    * similarity threshold - this is randomly chosen to be 0.5 but needs to be adjusted with evaluations of the results.\n",
    "* We compute the cosine similarity between the mentor profiles and mentee profile, we output the ones with similarity value greater than threshold\n",
    "\n",
    "Recommendations evaluation\n",
    "* As first part of evaluations we need to create ground truth:\n",
    "    * Pick sample users, say 100\n",
    "    * Recruit evaluators, say 3 (atleast more than 2 to cover for evaluator's subjective bias)\n",
    "    * Run one query (user preference) per evaluator.\n",
    "    * For this query mark each sampled user as relevant or not relevant. This can be done by looking up author's name on scholar websites to retrieve their interests and other profile details.\n",
    "* These scores can be used to fit a model on probability of relevant value, for P(relevant)>0.5 we can use the corresponding cosine similarity threshold values.\n",
    "* With ground truth, we can use IR evaluation methods like Precision, recall and RMSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Thank you"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "extensions": {
   "jupyter_dashboards": {
    "activeView": "grid_default",
    "version": 1,
    "views": {
     "grid_default": {
      "cellMargin": 10,
      "defaultCellHeight": 20,
      "maxColumns": 12,
      "name": "grid",
      "type": "grid"
     },
     "report_default": {
      "name": "report",
      "type": "report"
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
